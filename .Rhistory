library(e1071)
library(pROC)     # plot ROC
library(ROSE)     # for oversampling
# read data and split into train and test =====================================
# remember to remove duration
dat <- fread("bank/bank-full.csv",
stringsAsFactors = TRUE) %>%
.[, y := factor(y, levels = c("yes", "no"))] %>%
.[, duration := NULL] %>%
.[, month := NULL] %>%
.[, day := NULL] %>%
# randomForest() hates "-" in column names (happens after creating dummies)
.[, job := factor(stringr::str_replace_all(job, "-", "_"))]
# train-test split based on time. the last 9000 samples in test
dat_train <- dat[1:36211,]
dat_test <- dat[36212:45211,]
clean_data <- function(data = dat_train){
# To treat categorical feature. The unknowns are treated as its own category
# as there are reasons to be missing.
#
# Arguments
# ---------
#   data: data frame with features to be preprocessed
#
# Return
# ------
#   A data frame with preproccessed features
# pdays into categorical feature pcontact of two groups
data[, pdays := factor(ifelse(pdays == -1, "no", "yes"))]
# previous into "yes" and "no" and is exactly the same as pdays, delete
data[, previous := NULL]
return(data)
}
dat_train <- clean_data(dat_train)
dat_test <- clean_data(dat_test)
# fix skewness
pre_proc <- preProcess(dat_train, method = c("center", "scale", "BoxCox"))
dat_train <- predict(pre_proc, dat_train)
dat_test <- predict(pre_proc, dat_test)
# over-sampling train data
train_os <- ovun.sample(
y ~.,
data = dat_train,
method = "over",  # under or over
seed = 4321
)$data %>%
setDT()
# crate dummy variables
train <- create_dummies(dat_train)
test <- create_dummies(dat_train, dat_test)
train_os <- create_dummies(dat_train, train_os) %>%
.[, y := factor(y, levels = c("yes", "no"))]
# use the optimal max_node to train a random forest model
rf <- randomForest(y ~ ., train_os,
#maxnodes = 32,
ntree = 2000)
object.size(rf)
rc()
gc()
rf_pred_train <- predict(rf, train, type = "prob")[, "yes"]
rf_roc_train <- roc(train$y, rf_pred_train)
rf_pred_test <- predict(rf, test, type = "prob")[, "yes"]
rf_roc_test <- roc(test$y, rf_pred_test)
plot_rocs(`rf train` = rf_roc_train, `rf test` = rf_roc_test)
source("utilities.R")
library(randomForest)
library(xgboost)
library(e1071)
library(pROC)     # plot ROC
library(ROSE)     # for oversampling
# read data and split into train and test =====================================
# remember to remove duration
dat <- fread("bank/bank-full.csv",
stringsAsFactors = TRUE) %>%
.[, y := factor(y, levels = c("yes", "no"))] %>%
.[, duration := NULL] %>%
.[, month := NULL] %>%
.[, day := NULL] %>%
# randomForest() hates "-" in column names (happens after creating dummies)
.[, job := factor(stringr::str_replace_all(job, "-", "_"))]
# train-test split based on time. the last 9000 samples in test
dat_train <- dat[1:36211,]
dat_test <- dat[36212:45211,]
clean_data <- function(data = dat_train){
# To treat categorical feature. The unknowns are treated as its own category
# as there are reasons to be missing.
#
# Arguments
# ---------
#   data: data frame with features to be preprocessed
#
# Return
# ------
#   A data frame with preproccessed features
# pdays into categorical feature pcontact of two groups
data[, pdays := factor(ifelse(pdays == -1, "no", "yes"))]
# previous into "yes" and "no" and is exactly the same as pdays, delete
data[, previous := NULL]
return(data)
}
dat_train <- clean_data(dat_train)
dat_test <- clean_data(dat_test)
# fix skewness
pre_proc <- preProcess(dat_train, method = c("center", "scale", "BoxCox"))
dat_train <- predict(pre_proc, dat_train)
dat_test <- predict(pre_proc, dat_test)
# over-sampling train data
train_os <- ovun.sample(
y ~.,
data = dat_train,
method = "over",  # under or over
seed = 4321
)$data %>%
setDT()
# crate dummy variables
train <- create_dummies(dat_train)
test <- create_dummies(dat_train, dat_test)
train_os <- create_dummies(dat_train, train_os) %>%
.[, y := factor(y, levels = c("yes", "no"))]
rf <- train(
y ~.,
data = train_os[sample(1:nrow(train_os), 5000)],
method = "rf",
#maxnodes = max_nodes,
metric = "ROC",
trControl = train_control,
tuneGrid = tune_grid
)
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,  # for metric = "ROC"
summaryFunction=twoClassSummary,  # for metric "ROC"
verboseIter = FALSE
)
tune_grid <- expand.grid(mtry = 5:7)
# use a subset of train_os to quickly find the optimal max_node of random forest
set.seed(1111)
for (max_nodes in 2^(2:10)){
cat(paste0("\nmaxnodes = ", max_nodes, ":\n"))
rf <- train(
y ~.,
data = train_os[sample(1:nrow(train_os), 5000)],
method = "rf",
#maxnodes = max_nodes,
metric = "ROC",
trControl = train_control,
tuneGrid = tune_grid
)
print(rf$results)
}
# use a subset of train_os to quickly find the optimal max_node of random forest
set.seed(1111)
rf <- train(
y ~.,
data = train_os[sample(1:nrow(train_os), 5000)],
method = "rf",
#maxnodes = max_nodes,
metric = "ROC",
trControl = train_control,
tuneGrid = tune_grid
)
print(rf$results)
rf_pred_train <- predict(rf, train, type = "prob")[, "yes"]
rf_roc_train <- roc(train$y, rf_pred_train)
rf_pred_test <- predict(rf, test, type = "prob")[, "yes"]
rf_roc_test <- roc(test$y, rf_pred_test)
plot_rocs(`rf train` = rf_roc_train, `rf test` = rf_roc_test)
plot_lifts(test$y, rf_test = rf_pred_test)
# use the max_node to train a random forest model
rf <- randomForest(y ~ ., train_os,
maxnodes = 32,
ntree = 1000)
# read data and split into train and test =====================================
# remember to remove duration
dat <- fread("bank/bank-full.csv",
stringsAsFactors = TRUE) %>%
.[, y := factor(y, levels = c("yes", "no"))] %>%
.[, duration := NULL] %>%
.[, month := NULL] %>%
.[, day := NULL] %>%
# randomForest() hates "-" in column names (happens after creating dummies)
.[, job := factor(stringr::str_replace_all(job, "-", "_"))]
# train-test split based on time. the last 9000 samples in test
dat_train <- dat[1:36211,]
dat_test <- dat[36212:45211,]
clean_data <- function(data = dat_train){
# To treat categorical feature. The unknowns are treated as its own category
# as there are reasons to be missing.
#
# Arguments
# ---------
#   data: data frame with features to be preprocessed
#
# Return
# ------
#   A data frame with preproccessed features
# pdays into categorical feature pcontact of two groups
data[, pdays := factor(ifelse(pdays == -1, "no", "yes"))]
# previous into "yes" and "no" and is exactly the same as pdays, delete
data[, previous := NULL]
return(data)
}
dat_train <- clean_data(dat_train)
dat_test <- clean_data(dat_test)
# fix skewness
pre_proc <- preProcess(dat_train, method = c("center", "scale", "BoxCox"))
dat_train <- predict(pre_proc, dat_train)
dat_test <- predict(pre_proc, dat_test)
# over-sampling train data
train_os <- ovun.sample(
y ~.,
data = dat_train,
method = "over",  # under or over
seed = 4321
)$data %>%
setDT()
# crate dummy variables
train <- create_dummies(dat_train)
test <- create_dummies(dat_train, dat_test)
train_os <- create_dummies(dat_train, train_os) %>%
.[, y := factor(y, levels = c("yes", "no"))]
source("utilities.R")
# crate dummy variables
train <- create_dummies(dat_train)
test <- create_dummies(dat_train, dat_test)
train_os <- create_dummies(dat_train, train_os) %>%
.[, y := factor(y, levels = c("yes", "no"))]
source("utilities.R")
library(randomForest)
library(xgboost)
library(e1071)
library(pROC)     # plot ROC
library(ROSE)     # for oversampling
# read data and split into train and test =====================================
# remember to remove duration
dat <- fread("bank/bank-full.csv",
stringsAsFactors = TRUE) %>%
.[, y := factor(y, levels = c("yes", "no"))] %>%
.[, duration := NULL] %>%
.[, month := NULL] %>%
.[, day := NULL] %>%
# randomForest() hates "-" in column names (happens after creating dummies)
.[, job := factor(stringr::str_replace_all(job, "-", "_"))]
# train-test split based on time. the last 9000 samples in test
dat_train <- dat[1:36211,]
dat_test <- dat[36212:45211,]
clean_data <- function(data = dat_train){
# To treat categorical feature. The unknowns are treated as its own category
# as there are reasons to be missing.
#
# Arguments
# ---------
#   data: data frame with features to be preprocessed
#
# Return
# ------
#   A data frame with preproccessed features
# pdays into categorical feature pcontact of two groups
data[, pdays := factor(ifelse(pdays == -1, "no", "yes"))]
# previous into "yes" and "no" and is exactly the same as pdays, delete
data[, previous := NULL]
return(data)
}
dat_train <- clean_data(dat_train)
dat_test <- clean_data(dat_test)
# fix skewness
pre_proc <- preProcess(dat_train, method = c("center", "scale", "BoxCox"))
dat_train <- predict(pre_proc, dat_train)
dat_test <- predict(pre_proc, dat_test)
# over-sampling train data
train_os <- ovun.sample(
y ~.,
data = dat_train,
method = "over",  # under or over
seed = 4321
)$data %>%
setDT()
# crate dummy variables
train <- create_dummies(dat_train)
test <- create_dummies(dat_train, dat_test)
train_os <- create_dummies(dat_train, train_os) %>%
.[, y := factor(y, levels = c("yes", "no"))]
# use the max_node to train a random forest model
rf <- randomForest(y ~ ., train_os,
maxnodes = 32,
ntree = 1000)
rf_pred_train <- predict(rf, train, type = "prob")[, "yes"]
rf_roc_train <- roc(train$y, rf_pred_train)
rf_pred_test <- predict(rf, test, type = "prob")[, "yes"]
rf_roc_test <- roc(test$y, rf_pred_test)
plot_rocs(`rf train` = rf_roc_train, `rf test` = rf_roc_test)
plot_lifts(test$y, rf_test = rf_pred_test)
# read data, remove duration and change months
dat <- read.csv("bank/bank-full.csv",
stringsAsFactors = TRUE,
sep = ";")
dat$duration <- NULL
dat$month <- NULL
dat$day <- NULL
dat$y <- factor(dat$y, levels = c("yes", "no"))
dat_train <- dat[1:36211,]
dat_test <- dat[36212:45211,]
# baseline model: logistic regression
base <- glm(y ~ ., data = dat_train, family = binomial)
base_pred_train <- predict(base, dat_train, type = "response")
base_roc_train <- roc(dat_train$y, base_pred_train)
base_pred_test <- predict(base, dat_test, type = "response")
base_roc_test <- roc(dat_test$y, base_pred_test)
# plot roc curves using train and test data
plot_rocs(`base train` = base_roc_train, `base test` = base_roc_test)
plot_lifts(y = dat_test$y, `base_test` = 1 - base_pred_test) # prob for level 1
# logistic regression again ===================================================
lr <- glm(y ~ ., data = train_os, family = binomial)
lr_pred_train <- predict(lr, train, type = "response")
lr_roc_train <- roc(train$y, lr_pred_train)
lr_pred_test <- predict(lr, test, type = "response")
lr_roc_test <- roc(test$y, lr_pred_test)
# plot roc curves using train and test data
plot_rocs(`logit train` = lr_roc_train, `logit test` = lr_roc_test)
plot_lifts(test$y, logit_test = 1 - lr_pred_test)
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,  # for metric = "ROC"
summaryFunction=twoClassSummary,  # for metric "ROC"
verboseIter = TRUE
)
xgb <- train(
y ~.,
data = train_os,
method = "xgbTree",
# weights = ifelse(dat_train$y == "no", 0.1, 0.9),
metric = "ROC",
trControl = train_control,
tuneLength = 3
)
xgb_pred_train <- predict(xgb, dat_train, type = "prob")[, 2]
xgb_pred_test <- predict(xgb, dat_test, type = "prob")[, 2]
xgb$results
str(train_os)
# read data and split into train and test =====================================
# remember to remove duration
dat <- fread("bank/bank-full.csv",
stringsAsFactors = TRUE) %>%
.[, y := factor(y, levels = c("yes", "no"))] %>%
.[, duration := NULL] %>%
.[, month := NULL] %>%
.[, day := NULL] %>%
# randomForest() hates "-" in column names (happens after creating dummies)
.[, job := factor(stringr::str_replace_all(job, "-", ""))]
# train-test split based on time. the last 9000 samples in test
dat_train <- dat[1:36211,]
dat_test <- dat[36212:45211,]
clean_data <- function(data = dat_train){
# To treat categorical feature. The unknowns are treated as its own category
# as there are reasons to be missing.
#
# Arguments
# ---------
#   data: data frame with features to be preprocessed
#
# Return
# ------
#   A data frame with preproccessed features
# pdays into categorical feature pcontact of two groups
data[, pdays := factor(ifelse(pdays == -1, "no", "yes"))]
# previous into "yes" and "no" and is exactly the same as pdays, delete
data[, previous := NULL]
return(data)
}
dat_train <- clean_data(dat_train)
dat_test <- clean_data(dat_test)
# fix skewness
pre_proc <- preProcess(dat_train, method = c("center", "scale", "BoxCox"))
dat_train <- predict(pre_proc, dat_train)
dat_test <- predict(pre_proc, dat_test)
# over-sampling train data
train_os <- ovun.sample(
y ~.,
data = dat_train,
method = "over",  # under or over
seed = 4321
)$data %>%
setDT()
# crate dummy variables
train <- create_dummies(dat_train)
test <- create_dummies(dat_train, dat_test)
train_os <- create_dummies(dat_train, train_os) %>%
.[, y := factor(y, levels = c("yes", "no"))]
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,  # for metric = "ROC"
summaryFunction=twoClassSummary,  # for metric "ROC"
verboseIter = TRUE
)
xgb <- train(
y ~.,
data = train_os,
method = "xgbTree",
# weights = ifelse(dat_train$y == "no", 0.1, 0.9),
metric = "ROC",
trControl = train_control,
tuneLength = 3
)
xgb_pred_train <- predict(xgb, dat_train, type = "prob")[, 2]
str(train_os)
?dummyVars
create_dummies <- function(data_train, data_new = NULL, excluded = "y"){
# Create dummy variables for data_new using data_train
#
# Arguments:
#  data_train, data_new: data.tables
# exluded: vector of factor columns excluded from transfromation
#
# Returns:
#   Transformed dataframe of data_new
if (is.null(data_new)){
data_new <- data_train
}
cat_feats <- names(data_train)[sapply(data_train, is.factor)] %>%
setdiff(excluded)
other_feats <- setdiff(names(data_train), cat_feats)
train_cat <- data_train[, cat_feats, with = FALSE]
train_others <- data_train[, other_feats, with = FALSE]
new_cat <- data_new[, cat_feats, with = FALSE]
new_others <- data_new[, other_feats, with = FALSE]
dummy_trans <- dummyVars(~ ., train_cat, sep = "_", fullRank = TRUE)
new_dummies <- predict(dummy_trans, new_cat) %>%
as.data.table()
cbind(new_dummies, new_others)
}
# read data and split into train and test =====================================
# remember to remove duration
dat <- fread("bank/bank-full.csv",
stringsAsFactors = TRUE) %>%
.[, y := factor(y, levels = c("yes", "no"))] %>%
.[, duration := NULL] %>%
.[, month := NULL] %>%
.[, day := NULL] %>%
# randomForest() hates "-" in column names (happens after creating dummies)
.[, job := factor(stringr::str_replace_all(job, "-", ""))]
# train-test split based on time. the last 9000 samples in test
dat_train <- dat[1:36211,]
dat_test <- dat[36212:45211,]
clean_data <- function(data = dat_train){
# To treat categorical feature. The unknowns are treated as its own category
# as there are reasons to be missing.
#
# Arguments
# ---------
#   data: data frame with features to be preprocessed
#
# Return
# ------
#   A data frame with preproccessed features
# pdays into categorical feature pcontact of two groups
data[, pdays := factor(ifelse(pdays == -1, "no", "yes"))]
# previous into "yes" and "no" and is exactly the same as pdays, delete
data[, previous := NULL]
return(data)
}
dat_train <- clean_data(dat_train)
dat_test <- clean_data(dat_test)
# fix skewness
pre_proc <- preProcess(dat_train, method = c("center", "scale", "BoxCox"))
dat_train <- predict(pre_proc, dat_train)
dat_test <- predict(pre_proc, dat_test)
# over-sampling train data
train_os <- ovun.sample(
y ~.,
data = dat_train,
method = "over",  # under or over
seed = 4321
)$data %>%
setDT()
# crate dummy variables
train <- create_dummies(dat_train)
test <- create_dummies(dat_train, dat_test)
train_os <- create_dummies(dat_train, train_os) %>%
.[, y := factor(y, levels = c("yes", "no"))]
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,  # for metric = "ROC"
summaryFunction=twoClassSummary,  # for metric "ROC"
verboseIter = TRUE
)
train_control <- trainControl(
method = "cv",
number = 2,
search = "random",
classProbs = TRUE,  # for metric = "ROC"
summaryFunction=twoClassSummary,  # for metric "ROC"
verboseIter = TRUE
)
xgb <- train(
y ~.,
data = train_os,
method = "xgbTree",
# weights = ifelse(dat_train$y == "no", 0.1, 0.9),
metric = "ROC",
trControl = train_control,
tuneLength = 3
)
xgb_pred_train <- predict(xgb, dat_train, type = "prob")[, 2]
str(train_os)
